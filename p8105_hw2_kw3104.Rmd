---
title: "p8105_hw2_kw3104"
output: github_document
---

```{r setup}
library(tidyverse)
library(readxl)
library(dplyr)
```

## Problem 1
Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or case_match function may be useful).

```{r}
stations_data = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
  col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```
* This data is not tidy. To tidy up the data, I first load the NYC_Transit_Subway_Entrance_And_Exit_Data.csv file. I specify that `Route` columns 8-11 should be character for consistency with 1-7.  This dataset contain varibles like: division, Line, Station_name, Station_latitude, Station_longitude, Route1 to Route 11, Entrance_type, Entry, Exit Only, Vending, staffing, staff_hours, ADA, ADA_notes, Free_crossover, north_south_street, east_west_street, corner, entry_latitude, entrance_longitude, station_location and entrance_loctaion. Then I use `janitor` and `select` to chose those columns that I am interested. Also convert the entry variable from character `yes` vs `no` to logical variable `true` and `false`. There are 1868 rows and 20 columns of the resulting dataset. 


```{r}
stations_data %>%  
  select(station_name, line) %>%  
  distinct()
```
* There are 465 distinct stations.

```{r}
stations_data %>%  
  filter(ada == TRUE) %>%  
  select(station_name, line) %>%  
  distinct()
```
* There are 84 stations are ADA compliant.

```{r}
stations_data %>%  
  filter(vending == "NO") %>% 
  pull(entry) %>%  
  mean()
```
* There is 0.3770492 of station entrances / exits without vening allow entrance.

```{r}
stations_data %>%  
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>%  
  select(station_name, line) %>% 
  distinct()

stations_data %>%  
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>%  
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>%  
  distinct()
```
* There are 60 distinct stations serve the A train.
* Of the stations that serve the A train, 17 are ADA compliant. 


## Problem 2
Read and clean the Mr. Trash Wheel sheet:

specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
use reasonable variable names
omit rows that do not include dumpster-specific data
round the number of sports balls to the nearest integer and converts the result to an integer variable (using `as.integer`)

```{r}
Trash_data = "./data/202409 Trash Wheel Collection Data.xlsx"

Mr_Trash_Wheel = 
  read_excel(Trash_data, sheet = "Mr. Trash Wheel", range = "A2:N653") |> 
  janitor::clean_names() |>
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    year = as.double(year),
    Trash_Wheel = "Mr"
  )

Prof_Trash_Wheel = 
  read_excel(Trash_data, sheet = "Professor Trash Wheel", range = "A2:M120") |>
  janitor::clean_names() |>
  mutate(
    Trash_Wheel = "Prof"
    )

Gwynnda_Trash_Wheel = 
  read_excel(Trash_data, sheet = "Gwynnda Trash Wheel", range="A2:L265") |>
  janitor::clean_names() |>
  mutate(
    Trash_Wheel = "Gwynnda"
    )

combined_q2=bind_rows(Mr_Trash_Wheel,Prof_Trash_Wheel,Gwynnda_Trash_Wheel)
```
For `Mr_Trash_Wheel`, extract "Mr. Trash Wheel" from the excel, select range A2 to N653. Mutate `sports balls` in to interger and round them. Convert `year` from character to double. The dataset `Mr. Trash Wheel` has 651 observations and 15 variables. 

For `Prof_Trash_Wheel`, extract "Professor Trash Wheel" from the excel, select range A2 to M120.
The dataset `Professor Trash Wheel` has 118 observations and 14 variables.

For `Gwynnda_Trash_Wheel`, extract "Gwynnda Trash Wheel" from the excel, select range A2 to L265.
The dataset `Gwynnda Trash Wheel` has 263 observations and 13 variables.

The combined data of these three datasets `combined_q2` has 1032 observations and 15 variables. The combined dataset contains variable `dumpster` which indicates the number of dumpster and variables "year", "month", "date" and "weight_tons" etc. 

The total weight of trash collected by Professor Trash Wheel is: `r sum(combined_q2$weight_tons[combined_q2$Trash_Wheel=="Prof"])`, the total number of cigarette butts collected by Gwynnda in June of 2022 is: `r sum(combined_q2$cigarette_butts[combined_q2$Trash_Wheel=="Gwynnda" & combined_q2$year=="2022" & combined_q2$month=="June"])`.


## Problem 3
```{r}
Bakers = 
  read_csv("./data/gbb_datasets/bakers.csv") |>
  janitor::clean_names() |>
  separate(baker_name, into = c("baker", "last_name"), sep = " ") |>
  relocate(series)

Bakes = 
  read_csv("./data/gbb_datasets/bakes.csv") |>
  janitor::clean_names()

Results = 
  read_csv("./data/gbb_datasets/results.csv", skip = 2) |>
  janitor::clean_names()

anti = anti_join(Bakes, Results, by = c("series","episode","baker"))

bakes_results = full_join(Results, Bakes, by = c("series","episode","baker")) 
GBBO = 
left_join(bakes_results, Bakers, by = c("series", "baker")) |>
  arrange(series, episode)

write_csv(GBBO, "./data/GBBO.csv")
```

For `Bakers`, load the csv file and clean_name. Because the name of baker contain both first name and last name in one cell, I separate the first name into "baker" and last name into "last_name". Then, relocate "bakers" to the first column. The dataset `Bakers` has 120 observations and 6 variables.

For `Bakes`, load the csv file and clean_name. The dataset `Bakes` has 548 observations and 5 variables

For `Results`, load the csv file, skip the first two row that contain non-related information, then clean_name. the dataset `Results` has 1136 observations and 5 variables.

Use anti_join to check for completeness and correctness across datasets. 

full join `Bakers` and `Bakes` files by "series", "epidode" and "baker". Output dataset as `bakes_results`.
Left join `bakes_results` and `Bakers` by "series" and "baker". Arrange it by "series" and "episode". Output dataset as `GBBO`. The combined dataset of these three datasets `GBBO` has 1144 observations and 15 variables. The combined dataset contains variable "series", "episode", "baker" and "results", etc. 

Finaly, output `GBBO` to a csv file. 


```{r}
star_winner = GBBO |>
  filter((result == "WINNER" | result == "STAR BAKER")& series >= 5 & series <= 10) |>
  select(series, episode, baker, result) 
q2_table = as.table(as.matrix(star_winner))
q2_table
```

There were no predictable result for the final winner. Some person won "star baker" many times but didn't won the final "winner", and some person didn't won "star barker" on previous episode, but won the "winner" at the end. 

```{r}
Viewers = read_csv("./data/gbb_datasets/viewers.csv") |>
  janitor::clean_names()|>
  head(10)

average_series1 = mean(pull(Viewers, series_1), na.rm = TRUE)
average_series5 = mean(pull(Viewers, series_5))
```

Load the csv file, clean_name and use "head(10)" to show first 10 rows. 

The average viewership in Season 1 is 2.77.
The average viewership in Season 5 is 10.0393. 


  
  
  
  
```

